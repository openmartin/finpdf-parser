#!/usr/bin/env python3
"""
Qwen2-VL-2B-Instruct ONNX è½¬æ¢è„šæœ¬
å°†æ¨¡å‹è½¬æ¢ä¸ºä¸‰ä¸ª ONNX æ–‡ä»¶ï¼š
1. embed_tokens.onnx - æ–‡æœ¬åµŒå…¥å±‚
2. vision_encoder.onnx - è§†è§‰ç¼–ç å™¨
3. decoder_model_merged.onnx - å®Œæ•´è§£ç å™¨ï¼ˆTransformer + LM Head + KV Cacheï¼‰
"""

import os
import torch
import onnx
import onnxruntime as ort
from transformers import AutoConfig, AutoProcessor, Qwen2VLForConditionalGeneration
from transformers.image_utils import load_image
import numpy as np
from pathlib import Path
import traceback

# è®¾ç½® Hugging Face é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class Qwen2VLToONNXConverter:
    def __init__(self, model_id: str = "Qwen/Qwen2-VL-2B-Instruct", output_dir: str = "model/onnx"):
        self.model_id = model_id
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        print(f"åˆå§‹åŒ–è½¬æ¢å™¨...")
        print(f"æ¨¡å‹ID: {model_id}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

        self._load_model()

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")

        self.config = AutoConfig.from_pretrained(self.model_id)
        self.processor = AutoProcessor.from_pretrained(self.model_id)

        self.model = Qwen2VLForConditionalGeneration.from_pretrained(
            self.model_id,
            torch_dtype=torch.float32,
        ).eval()

        print("æ¨¡å‹åŠ è½½å®Œæˆ")

    def export_vision_encoder(self):
        """å¯¼å‡ºè§†è§‰ç¼–ç å™¨ä¸º ONNX"""
        print("æ­£åœ¨å¯¼å‡ºè§†è§‰ç¼–ç å™¨...")

        # å‡†å¤‡ç¤ºä¾‹è¾“å…¥ - ä½¿ç”¨æ ‡å‡†å›¾åƒå°ºå¯¸
        batch_size = 1
        num_images = 1
        dummy_pixel_values = torch.randn(
            batch_size, num_images, 3, 224, 224, 
            dtype=torch.float32, 
            device=self.model.device
        )
        dummy_pixel_attention_mask = torch.ones(
            batch_size, num_images, 224, 224, 
            dtype=torch.bool, 
            device=self.model.device
        )
        dummy_image_grid_thw = torch.ones(
            batch_size, num_images, 3, 
            dtype=torch.long, 
            device=self.model.device
        )
        dummy_image_grid_thw[:, :, 0] = 224  # height
        dummy_image_grid_thw[:, :, 1] = 224  # width

        # åˆ›å»ºè§†è§‰ç¼–ç å™¨åŒ…è£…ç±»
        class VisionWrapper(torch.nn.Module):
            def __init__(self, model):
                super().__init__()
                self.visual = model.visual

            def forward(self, pixel_values, image_grid_thw):
                # Qwen2VL çš„è§†è§‰ç¼–ç å™¨ç›´æ¥å¤„ç† pixel_values
                # éœ€è¦å°†è¾“å…¥ reshape ä¸ºæ­£ç¡®çš„æ ¼å¼
                batch_size, num_images, channels, height, width = pixel_values.shape
                pixel_values = pixel_values.view(batch_size * num_images, channels, height, width)

                # è°ƒç”¨è§†è§‰ç¼–ç å™¨çš„ patch_embed å’Œåç»­å¤„ç†
                hidden_states = self.visual.patch_embed(pixel_values)

                # å‡†å¤‡ rotary_pos_emb
                grid_thw = image_grid_thw.view(-1, 3)
                rotary_pos_emb = self.visual.rot_pos_emb(grid_thw)

                # é€šè¿‡æ‰€æœ‰å±‚
                cu_seqlens = torch.arange(
                    0, (batch_size * num_images + 1) * hidden_states.shape[1],
                    step=hidden_states.shape[1],
                    dtype=torch.int32,
                    device=hidden_states.device
                )

                for blk in self.visual.blocks:
                    hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)

                # åˆå¹¶æ—¶ç©ºç»´åº¦
                hidden_states = self.visual.merger(hidden_states)

                return hidden_states

        vision_wrapper = VisionWrapper(self.model)

        # å¯¼å‡ºè§†è§‰ç¼–ç å™¨
        output_path = self.output_dir / "vision_encoder.onnx"
        torch.onnx.export(
            vision_wrapper,
            (dummy_pixel_values, dummy_image_grid_thw),
            str(output_path),
            input_names=["pixel_values", "image_grid_thw"],
            output_names=["image_embeds"],
            dynamic_axes={
                "pixel_values": {0: "batch_size", 1: "num_images"},
                "image_grid_thw": {0: "batch_size", 1: "num_images"},
                "image_embeds": {0: "batch_size", 1: "seq_length"}
            },
            opset_version=18,
            do_constant_folding=True,
        )

        print(f"è§†è§‰ç¼–ç å™¨å¯¼å‡ºå®Œæˆ: {output_path}")

    def export_embed_tokens(self):
        """å¯¼å‡ºåµŒå…¥å±‚ä¸º ONNX"""
        print("æ­£åœ¨å¯¼å‡ºåµŒå…¥å±‚...")

        # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
        batch_size = 1
        seq_length = 512
        dummy_input_ids = torch.randint(
            0, self.config.text_config.vocab_size, 
            (batch_size, seq_length), 
            dtype=torch.long, 
            device=self.model.device
        )

        # åˆ›å»ºåµŒå…¥å±‚åŒ…è£…ç±»
        class EmbedTokensWrapper(torch.nn.Module):
            def __init__(self, model):
                super().__init__()
                # ä¿®æ­£: Qwen2VL çš„ embed_tokens ç›´æ¥åœ¨ language_model ä¸­
                self.embed_tokens = model.language_model.embed_tokens

            def forward(self, input_ids):
                return self.embed_tokens(input_ids)

        embed_wrapper = EmbedTokensWrapper(self.model)

        # å¯¼å‡ºåµŒå…¥å±‚
        output_path = self.output_dir / "embed_tokens.onnx"
        torch.onnx.export(
            embed_wrapper,
            dummy_input_ids,
            str(output_path),
            input_names=["input_ids"],
            output_names=["inputs_embeds"],
            dynamic_axes={
                "input_ids": {0: "batch_size", 1: "seq_length"},
                "inputs_embeds": {0: "batch_size", 1: "seq_length"}
            },
            opset_version=18,
            do_constant_folding=True,
        )

        print(f"åµŒå…¥å±‚å¯¼å‡ºå®Œæˆ: {output_path}")

    def export_decoder_model_merged(self):
        """å¯¼å‡ºå®Œæ•´çš„è§£ç å™¨æ¨¡å‹ï¼ˆåŒ…å« Transformer + LM Head + KV Cacheï¼‰ä¸º ONNX"""
        print("æ­£åœ¨å¯¼å‡ºè§£ç å™¨æ¨¡å‹...")

        # å‡†å¤‡ç¤ºä¾‹è¾“å…¥
        batch_size = 1
        seq_length = 1  # ç”Ÿæˆé˜¶æ®µé€šå¸¸æ˜¯å• token
        num_key_value_heads = self.config.text_config.num_key_value_heads
        head_dim = self.config.text_config.hidden_size // self.config.text_config.num_attention_heads
        num_hidden_layers = self.config.text_config.num_hidden_layers

        dummy_inputs_embeds = torch.randn(
            batch_size, seq_length, self.config.text_config.hidden_size,
            dtype=torch.float32, 
            device=self.model.device
        )
        dummy_attention_mask = torch.ones(
            batch_size, seq_length, 
            dtype=torch.long, 
            device=self.model.device
        )

        # å¯é€‰çš„è§†è§‰è¾“å…¥å‚æ•°ï¼ˆç”¨äºæ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼‰
        dummy_image_embeds = torch.randn(
            batch_size, 1, self.config.vision_config.hidden_size,
            dtype=torch.float32, 
            device=self.model.device
        )
        dummy_image_grid_thw = torch.ones(
            batch_size, 1, 3, 
            dtype=torch.long, 
            device=self.model.device
        )
        dummy_image_grid_thw[:, :, 0] = 224  # height
        dummy_image_grid_thw[:, :, 1] = 224  # width

        # åˆ›å»º past_key_values
        past_key_values = tuple([
            (
                torch.zeros(batch_size, num_key_value_heads, 0, head_dim, 
                           dtype=torch.float32, device=self.model.device),
                torch.zeros(batch_size, num_key_value_heads, 0, head_dim, 
                           dtype=torch.float32, device=self.model.device)
            )
            for _ in range(num_hidden_layers)
        ])

        # åˆ›å»ºè§£ç å™¨åŒ…è£…ç±»
        class DecoderWrapper(torch.nn.Module):
            def __init__(self, model):
                super().__init__()
                # ä¿®æ­£:ä½¿ç”¨ language_model è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹
                self.model = model.language_model

            def forward(self, inputs_embeds, attention_mask, image_embeds=None, image_grid_thw=None):
                # å§‹ç»ˆä»¥æ— ç¼“å­˜å¼€å§‹ï¼Œé¿å…ä¼ å…¥ tuple å¯¼è‡´ get_seq_length æŠ¥é”™
                model_kwargs = {
                    "inputs_embeds": inputs_embeds,
                    "attention_mask": attention_mask,
                    "past_key_values": None,
                    "use_cache": True,
                    "return_dict": True
                }

                # æ·»åŠ è§†è§‰å‚æ•°ï¼ˆå¦‚æä¾›ï¼‰
                if image_embeds is not None and image_grid_thw is not None:
                    model_kwargs["image_embeds"] = image_embeds
                    model_kwargs["image_grid_thw"] = image_grid_thw

                outputs = self.model(**model_kwargs)

                # è¿”å› logits å’Œå±•å¹³çš„ present_key_values
                logits = outputs.logits
                present_kvs = []
                for key, value in outputs.past_key_values:
                    present_kvs.extend([key, value])

                return tuple([logits] + present_kvs)

        decoder_wrapper = DecoderWrapper(self.model)

        # å‡†å¤‡è¾“å…¥è¾“å‡ºåç§°ï¼ˆä¸å†å°† past ä½œä¸ºè¾“å…¥ï¼‰
        input_names = ["inputs_embeds", "attention_mask", "image_embeds", "image_grid_thw"]
        output_names = ["logits"]
        for i in range(num_hidden_layers):
            output_names.extend([f"present.{i}.key", f"present.{i}.value"])

        # ä½¿ç”¨ dynamic_shapesï¼ˆåŒ¹é… 4 ä¸ªè¾“å…¥çš„ç»“æ„ï¼‰
        from torch.export import Dim
        batch = Dim("batch_size")
        seq = Dim("seq_length")
        total_seq = Dim("total_seq_length")
        num_imgs = Dim("num_images")

        dynamic_shapes = (
            {0: batch, 1: seq},        # inputs_embeds: [B, T, H]
            {0: batch, 1: total_seq},  # attention_mask: [B, T_total]
            {0: batch, 1: num_imgs},   # image_embeds: [B, N, H_v]
            {0: batch, 1: num_imgs},   # image_grid_thw: [B, N, 3]
        )

        # å¯¼å‡ºè§£ç å™¨
        output_path = self.output_dir / "decoder_model_merged.onnx"
        torch.onnx.export(
            decoder_wrapper,
            (dummy_inputs_embeds, dummy_attention_mask, dummy_image_embeds, dummy_image_grid_thw),
            str(output_path),
            input_names=input_names,
            output_names=output_names,
            dynamic_shapes=dynamic_shapes,
            opset_version=18,
            do_constant_folding=True,
        )

        print(f"è§£ç å™¨æ¨¡å‹å¯¼å‡ºå®Œæˆ: {output_path}")

    def validate_onnx_models(self):
        """éªŒè¯å¯¼å‡ºçš„ ONNX æ¨¡å‹"""
        print("æ­£åœ¨éªŒè¯ ONNX æ¨¡å‹...")

        models_to_check = [
            "embed_tokens.onnx",
            "vision_encoder.onnx",
            "decoder_model_merged.onnx"
        ]

        all_valid = True
        for model_name in models_to_check:
            model_path = self.output_dir / model_name
            if model_path.exists():
                try:
                    onnx_model = onnx.load(str(model_path))
                    onnx.checker.check_model(onnx_model)

                    # æ£€æŸ¥æ¨¡å‹å¤§å°
                    file_size_mb = model_path.stat().st_size / (1024 * 1024)
                    print(f"âœ“ {model_name} éªŒè¯é€šè¿‡ (å¤§å°: {file_size_mb:.1f} MB)")
                except Exception as e:
                    print(f"âœ— {model_name} éªŒè¯å¤±è´¥: {e}")
                    all_valid = False
            else:
                print(f"âœ— {model_name} æ–‡ä»¶ä¸å­˜åœ¨")
                all_valid = False

        return all_valid

    def convert_all(self):
        """æ‰§è¡Œå®Œæ•´çš„è½¬æ¢æµç¨‹"""
        print("å¼€å§‹ Qwen2-VL-2B-Instruct ONNX è½¬æ¢æµç¨‹...")

        try:
            # å¯¼å‡ºå„ä¸ªç»„ä»¶
            # self.export_embed_tokens()
            # self.export_vision_encoder()
            self.export_decoder_model_merged()

            # éªŒè¯æ¨¡å‹
            if self.validate_onnx_models():
                print("\nğŸ‰ æ‰€æœ‰ ONNX æ¨¡å‹è½¬æ¢æˆåŠŸå®Œæˆï¼")
                print(f"æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: {self.output_dir.absolute()}")

                # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
                print("\nç”Ÿæˆçš„æ–‡ä»¶:")
                for file_path in self.output_dir.glob("*.onnx"):
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    print(f"  - {file_path.name} ({size_mb:.1f} MB)")
            else:
                print("\nâŒ æ¨¡å‹éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è½¬æ¢è¿‡ç¨‹")

        except Exception as e:
            print(f"\nâŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è‡ªå®šä¹‰æ¨¡å‹IDå’Œè¾“å‡ºç›®å½•
    model_id = os.getenv("MODEL_ID", "Qwen/Qwen2-VL-2B-Instruct")
    output_dir = os.getenv("OUTPUT_DIR", "onnx-model")

    converter = Qwen2VLToONNXConverter(model_id, output_dir)
    converter.convert_all()

if __name__ == "__main__":
    main()
